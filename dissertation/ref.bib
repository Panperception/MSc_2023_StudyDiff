@inproceedings{alexnet_neurips_krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2023-08-20},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\YLRUUR7J\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}
% == BibTeX quality report for alexnet_neurips_krizhevskyImageNetClassificationDeep2012:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Neural Information Processing Systems")

@misc{anime_dataset_HighResolutionAnimeFace,
  title = {High-{{Resolution Anime Face Dataset}} (512x512)},
  author = {An, Subin},
  year = {2021},
  url = {https://www.kaggle.com/datasets/subinium/highresolution-anime-face-dataset-512x512},
  urldate = {2023-09-05},
  abstract = {Anime face-specific high-resolution dataset from danbooru},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\QF54JTMC\\highresolution-anime-face-dataset-512x512.html}
}
% == BibTeX quality report for anime_dataset_HighResolutionAnimeFace:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{attention_is_all_you_need_NIPS2017_3f5ee243,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}
% == BibTeX quality report for attention_is_all_you_need_NIPS2017_3f5ee243:
% ? Unsure about the formatting of the booktitle

@inproceedings{autoregressive_model_1_choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}\textendash{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and {van Merri{\"e}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = oct,
  pages = {1724--1734},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1179},
  url = {https://aclanthology.org/D14-1179},
  urldate = {2023-08-27},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\44ZE57N5\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoderâ€“.pdf}
}
% == BibTeX quality report for autoregressive_model_1_choLearningPhraseRepresentations2014:
% ? unused Conference name ("EMNLP 2014")
% ? unused Library catalog ("ACLWeb")

@misc{autoregressive_model_2_oordPixelRecurrentNeural2016b,
  title = {Pixel {{Recurrent Neural Networks}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year = {2016},
  month = aug,
  number = {arXiv:1601.06759},
  eprint = {1601.06759},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1601.06759},
  url = {http://arxiv.org/abs/1601.06759},
  urldate = {2023-08-27},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\AQXZJJA9\\Oord et al. - 2016 - Pixel Recurrent Neural Networks.pdf;C\:\\Users\\hongz\\Zotero\\storage\\BFRT89DE\\1601.html}
}
% == BibTeX quality report for autoregressive_model_2_oordPixelRecurrentNeural2016b:
% ? Title looks like it was stored in title-case in Zotero

@misc{autoregressive_model_3_salimansPixelCNNImprovingPixelCNN2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  year = {2017},
  month = jan,
  number = {arXiv:1701.05517},
  eprint = {1701.05517},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1701.05517},
  url = {http://arxiv.org/abs/1701.05517},
  urldate = {2023-08-27},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\GFXS9V82\\Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretize.pdf;C\:\\Users\\hongz\\Zotero\\storage\\PQMMKKIM\\1701.html}
}
% == BibTeX quality report for autoregressive_model_3_salimansPixelCNNImprovingPixelCNN2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{beta_vae_higginsBetaVAELearningBasic2016,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  shorttitle = {Beta-{{VAE}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year = {2016},
  month = nov,
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate = {2023-08-23},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta {$>$} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\UZL42V8M\\Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf}
}
% == BibTeX quality report for beta_vae_higginsBetaVAELearningBasic2016:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("openreview.net")

@inproceedings{big_gan_iclr_brockLargeScaleGAN2018,
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  year = {2018},
  month = sep,
  url = {https://openreview.net/forum?id=B1xsqj09Fm},
  urldate = {2023-08-23},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick", allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\QBK4GJWI\\Brock et al. - 2018 - Large Scale GAN Training for High Fidelity Natural.pdf}
}
% == BibTeX quality report for big_gan_iclr_brockLargeScaleGAN2018:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("openreview.net")

@misc{conditional_gan_mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  year = {2014},
  month = nov,
  number = {arXiv:1411.1784},
  eprint = {1411.1784},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1411.1784},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2023-08-23},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\QEHB64N6\\Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;C\:\\Users\\hongz\\Zotero\\storage\\LUN9RIZX\\1411.html}
}
% == BibTeX quality report for conditional_gan_mirzaConditionalGenerativeAdversarial2014:
% ? Title looks like it was stored in title-case in Zotero

@misc{cosine_annealing_loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  month = may,
  number = {arXiv:1608.03983},
  eprint = {1608.03983},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1608.03983},
  url = {http://arxiv.org/abs/1608.03983},
  urldate = {2023-09-06},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\HRD6VUHE\\Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf;C\:\\Users\\hongz\\Zotero\\storage\\SLN6FWZK\\1608.html}
}
% == BibTeX quality report for cosine_annealing_loshchilovSGDRStochasticGradient2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Version number ("5")

@inproceedings{cvae_nips_NIPS2015_8d55a249,
  title = {Learning Structured Output Representation Using Deep Conditional Generative Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf}
}
% == BibTeX quality report for cvae_nips_NIPS2015_8d55a249:
% ? Unsure about the formatting of the booktitle

@misc{dalle2_openai_rameshHierarchicalTextConditionalImage2022a,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2023-08-27},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\SEU2T3LJ\\Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf;C\:\\Users\\hongz\\Zotero\\storage\\NT7ZA5XT\\2204.html}
}
% == BibTeX quality report for dalle2_openai_rameshHierarchicalTextConditionalImage2022a:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Version number ("1")

@inproceedings{ddpm_hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2023-08-20},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\PPNGWRNG\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}
% == BibTeX quality report for ddpm_hoDenoisingDiffusionProbabilistic2020:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Neural Information Processing Systems")

@misc{ddpm_math_luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {2208.11970},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.11970},
  urldate = {2023-08-26},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\LSWBHFLB\\Luo - 2022 - Understanding Diffusion Models A Unified Perspect.pdf;C\:\\Users\\hongz\\Zotero\\storage\\DW4M2EW5\\2208.html}
}
% == BibTeX quality report for ddpm_math_luoUnderstandingDiffusionModels2022:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{deep_convolutional_gan_iclr_radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2023-08-22}
}
% == BibTeX quality report for deep_convolutional_gan_iclr_radfordUnsupervisedRepresentationLearning2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DBLP Computer Science Bibliography")

@inproceedings{diffusion_Nonequilibrium_Thermodynamics_sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = jun,
  pages = {2256--2265},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  urldate = {2023-09-05},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\V53897IF\\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}
% == BibTeX quality report for diffusion_Nonequilibrium_Thermodynamics_sohl-dicksteinDeepUnsupervisedLearning2015:
% ? unused Conference name ("International Conference on Machine Learning")
% ? unused Library catalog ("proceedings.mlr.press")

@inproceedings{discrete_vae_rolfeDiscreteVariationalAutoencoders2016,
  title = {Discrete {{Variational Autoencoders}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Rolfe, Jason Tyler},
  year = {2016},
  month = nov,
  url = {https://openreview.net/forum?id=ryMxXPFex},
  urldate = {2023-08-23},
  abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\7ND27MJG\\Rolfe - 2016 - Discrete Variational Autoencoders.pdf}
}
% == BibTeX quality report for discrete_vae_rolfeDiscreteVariationalAutoencoders2016:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("openreview.net")

@misc{fid_heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2018},
  month = jan,
  number = {arXiv:1706.08500},
  eprint = {1706.08500},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.08500},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2023-09-05},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\X5DR9I6D\\Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf;C\:\\Users\\hongz\\Zotero\\storage\\ZFD8EQV9\\1706.html}
}
% == BibTeX quality report for fid_heuselGANsTrainedTwo2018:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{gan_NIPS2014_5ca3e9b1,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf}
}
% == BibTeX quality report for gan_NIPS2014_5ca3e9b1:
% ? Unsure about the formatting of the booktitle

@misc{generative_model_image_GenerativeModels,
  title = {Generative Models},
  author = {OpenAI},
  url = {https://openai.com/research/generative-models},
  urldate = {2023-08-31},
  abstract = {This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning. In addition to describing our work, this post will tell you a bit more about generative models: what they are, why they are important, and where they might be going.},
  langid = {american},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\4CKGGP7S\\generative-models.html}
}

@article{gpt1_radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\VG2JSDUU\\Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}
% == BibTeX quality report for gpt1_radfordImprovingLanguageUnderstanding2018:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")

@article{gpt2_radfordLanguageModelsAre2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\JIBQL49N\\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}
% == BibTeX quality report for gpt2_radfordLanguageModelsAre2019:
% Missing required field 'journal'
% ? unused Library catalog ("Zotero")

@inproceedings{gpt3_brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2023-09-05},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\FKLDBY7F\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}
% == BibTeX quality report for gpt3_brownLanguageModelsAre2020:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Neural Information Processing Systems")

@misc{gpt4_openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08774},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2023-09-05},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\RGPQZVVT\\OpenAI - 2023 - GPT-4 Technical Report.pdf;C\:\\Users\\hongz\\Zotero\\storage\\XYPK8HY6\\2303.html}
}
% == BibTeX quality report for gpt4_openaiGPT4TechnicalReport2023:
% ? Title looks like it was stored in title-case in Zotero

@misc{group_normalization_wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  year = {2018},
  month = jun,
  number = {arXiv:1803.08494},
  eprint = {1803.08494},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.08494},
  url = {http://arxiv.org/abs/1803.08494},
  urldate = {2023-09-08},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\MFS8TX57\\Wu and He - 2018 - Group Normalization.pdf;C\:\\Users\\hongz\\Zotero\\storage\\VEJEAURR\\1803.html}
}
% == BibTeX quality report for group_normalization_wuGroupNormalization2018:
% ? Title looks like it was stored in title-case in Zotero

@misc{hugging_face_butterfly_dataset_CeydaSmithsonian_butterfliesDatasets2023a,
  title = {Ceyda/Smithsonian\_butterflies {$\cdot$} {{Datasets}} at {{Hugging Face}}},
  author = {Cinarel, Ceyda},
  year = {2023},
  month = apr,
  url = {https://huggingface.co/datasets/ceyda/smithsonian_butterflies},
  urldate = {2023-09-05},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\IZALHU4I\\smithsonian_butterflies.html}
}

@misc{image_data_augmentation_yangImageDataAugmentation2022,
  title = {Image {{Data Augmentation}} for {{Deep Learning}}: {{A Survey}}},
  shorttitle = {Image {{Data Augmentation}} for {{Deep Learning}}},
  author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
  year = {2022},
  month = apr,
  number = {arXiv:2204.08610},
  eprint = {2204.08610},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.08610},
  url = {http://arxiv.org/abs/2204.08610},
  urldate = {2023-09-06},
  abstract = {Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overfitting. However, labeled data for real-world applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data. As an effective way to improve the sufficiency and diversity of training data, data augmentation has become a necessary part of successful application of deep learning models on image data. In this paper, we systematically review different image data augmentation methods. We propose a taxonomy of reviewed methods and present the strengths and limitations of these methods. We also conduct extensive experiments with various data augmentation methods on three typical computer vision tasks, including semantic segmentation, image classification and object detection. Finally, we discuss current challenges faced by data augmentation and future research directions to put forward some useful research guidance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\NSKV2F9K\\Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf;C\:\\Users\\hongz\\Zotero\\storage\\QWWUBA34\\2204.html}
}
% == BibTeX quality report for image_data_augmentation_yangImageDataAugmentation2022:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{image_net_cvpr_dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500\textendash 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\BUNF8R23\\Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf;C\:\\Users\\hongz\\Zotero\\storage\\5CXB2C7C\\5206848.html}
}
% == BibTeX quality report for image_net_cvpr_dengImageNetLargescaleHierarchical2009:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("IEEE Xplore")

@misc{imagen_google_sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2205.11487},
  eprint = {2205.11487},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11487},
  url = {http://arxiv.org/abs/2205.11487},
  urldate = {2023-08-27},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\ISD69TMP\\Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf;C\:\\Users\\hongz\\Zotero\\storage\\FRERQSIL\\2205.html}
}
% == BibTeX quality report for imagen_google_sahariaPhotorealisticTexttoImageDiffusion2022:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{inception_v3_szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  year = {2016},
  pages = {2818--2826},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html},
  urldate = {2023-09-05},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\C6J7ZAPB\\Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf}
}
% == BibTeX quality report for inception_v3_szegedyRethinkingInceptionArchitecture2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("openaccess.thecvf.com")

@inproceedings{latent_diffusion_cvpr_rombachHighResolutionImageSynthesis2022a,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
  year = {2022},
  month = jun,
  pages = {10674--10685},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01042},
  url = {https://ieeexplore.ieee.org/document/9878449/},
  urldate = {2023-08-27},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\E8UDN2NF\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}
% == BibTeX quality report for latent_diffusion_cvpr_rombachHighResolutionImageSynthesis2022a:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")

@article{normalizing_flow_variational_autoencoder_morrowVariationalAutoencodersNormalizing2019,
  title = {Variational {{Autoencoders}} with {{Normalizing Flow Decoders}}},
  author = {Morrow, Rogan and Chiu, Wei-Chen},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=r1eh30NFwB},
  urldate = {2023-08-23},
  abstract = {Recently proposed normalizing flow models such as Glow (Kingma \& Dhariwal, 2018) have been shown to be able to generate high quality, high dimensional images with relatively fast sampling speed. Due to the inherently restrictive design of architecture , however, it is necessary that their model are excessively deep in order to achieve effective training. In this paper we propose to combine Glow model with an underlying variational autoencoder in order to counteract this issue. We demonstrate that such our proposed model is competitive with Glow in terms of image quality while requiring far less time for training. Additionally, our model achieves state-of-the-art FID score on CIFAR-10 for a likelihood-based model.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\V39HURTR\\Morrow and Chiu - 2019 - Variational Autoencoders with Normalizing Flow Dec.pdf}
}
% == BibTeX quality report for normalizing_flow_variational_autoencoder_morrowVariationalAutoencodersNormalizing2019:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("openreview.net")

@inproceedings{oxford_flower_nilsbackAutomatedFlowerClassification2008,
  title = {Automated {{Flower Classification}} over a {{Large Number}} of {{Classes}}},
  booktitle = {2008 {{Sixth Indian Conference}} on {{Computer Vision}}, {{Graphics}} \& {{Image Processing}}},
  author = {Nilsback, Maria-Elena and Zisserman, Andrew},
  year = {2008},
  month = dec,
  pages = {722--729},
  publisher = {{IEEE}},
  address = {{Bhubaneswar, India}},
  doi = {10.1109/ICVGIP.2008.47},
  url = {http://ieeexplore.ieee.org/document/4756141/},
  urldate = {2023-08-19},
  abstract = {We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray [16], which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1\% for the best single feature to 72.8\% for the combination of all features.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\WAEMFYTM\\Nilsback and Zisserman - 2008 - Automated Flower Classification over a Large Numbe.pdf}
}
% == BibTeX quality report for oxford_flower_nilsbackAutomatedFlowerClassification2008:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("Image Processing (ICVGIP)")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{progressive_gan_iclr_karrasProgressiveGrowingGANs2018,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  month = feb,
  url = {https://openreview.net/forum?id=Hk99zCeAb&},
  urldate = {2023-08-23},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024\^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\5SMEYSHX\\Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf}
}
% == BibTeX quality report for progressive_gan_iclr_karrasProgressiveGrowingGANs2018:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("openreview.net")

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2023-08-27},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\CHKF6JL4\\Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf;C\:\\Users\\hongz\\Zotero\\storage\\IR24XU66\\2204.html}
}
% == BibTeX quality report for rameshHierarchicalTextConditionalImage2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Version number ("1")

@inproceedings{resnet_heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  url = {http://ieeexplore.ieee.org/document/7780459/},
  urldate = {2023-08-26},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\textemdash 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\DG8MEIC3\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}
% == BibTeX quality report for resnet_heDeepResidualLearning2016:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")

@article{rnn_lstm_sherstinskyFundamentalsRecurrentNeural2020,
  title = {Fundamentals of {{Recurrent Neural Network}} ({{RNN}}) and {{Long Short-Term Memory}} ({{LSTM}}) Network},
  author = {Sherstinsky, Alex},
  year = {2020},
  month = mar,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {404},
  pages = {132306},
  issn = {0167-2789},
  doi = {10.1016/j.physd.2019.132306},
  url = {https://www.sciencedirect.com/science/article/pii/S0167278919305974},
  urldate = {2023-08-22},
  abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of ``unrolling'' an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the ``Vanilla LSTM''1 1The nickname ``Vanilla LSTM'' symbolizes this model's flexibility and generality (Greff et~al., 2015). network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
  keywords = {Convolutional input context windows,External input gate,LSTM,RNN,RNN unfolding/unrolling},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\HJKP6G8M\\Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf;C\:\\Users\\hongz\\Zotero\\storage\\UPVWKAGF\\S0167278919305974.html}
}
% == BibTeX quality report for rnn_lstm_sherstinskyFundamentalsRecurrentNeural2020:
% ? unused Library catalog ("ScienceDirect")

@misc{score_base_songScoreBasedGenerativeModeling2021a,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13456},
  eprint = {2011.13456},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.13456},
  url = {http://arxiv.org/abs/2011.13456},
  urldate = {2023-09-05},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\textbackslash aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\H4AHLLIV\\Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf;C\:\\Users\\hongz\\Zotero\\storage\\KSXK4GZE\\2011.html}
}
% == BibTeX quality report for score_base_songScoreBasedGenerativeModeling2021a:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{sequential_variational_autoencoder_sachdevaSequentialVariationalAutoencoders2019,
  title = {Sequential {{Variational Autoencoders}} for {{Collaborative Filtering}}},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Sachdeva, Noveen and Manco, Giuseppe and Ritacco, Ettore and Pudi, Vikram},
  year = {2019},
  month = jan,
  series = {{{WSDM}} '19},
  pages = {600--608},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3289600.3291007},
  url = {https://dl.acm.org/doi/10.1145/3289600.3291007},
  urldate = {2023-08-23},
  abstract = {Variational autoencoders were proven successful in domains such as computer vision and speech processing. Their adoption for modeling user preferences is still unexplored, although recently it is starting to gain attention in the current literature. In this work, we propose a model which extends variational autoencoders by exploiting the rich information present in the past preference history. We introduce a recurrent version of the VAE, where instead of passing a subset of the whole history regardless of temporal dependencies, we rather pass the consumption sequence subset through a recurrent neural network. At each time-step of the RNN, the sequence is fed through a series of fully-connected layers, the output of which models the probability distribution of the most likely future preferences. We show that handling temporal information is crucial for improving the accuracy of the VAE: In fact, our model beats the current state-of-the-art by valuable margins because of its ability to capture temporal dependencies among the user-consumption sequence using the recurrent encoder still keeping the fundamentals of variational autoencoders intact.},
  isbn = {978-1-4503-5940-5},
  keywords = {recurrent networks,sequence modeling,variational autoencoders},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\B4S523M9\\Sachdeva et al. - 2019 - Sequential Variational Autoencoders for Collaborat.pdf}
}
% == BibTeX quality report for sequential_variational_autoencoder_sachdevaSequentialVariationalAutoencoders2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("ACM Digital Library")

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2023-08-22},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\M74SYNCL\\Sohn et al. - 2015 - Learning Structured Output Representation using De.pdf}
}
% == BibTeX quality report for sohnLearningStructuredOutput2015:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Neural Information Processing Systems")

@inproceedings{style_gan_1_karrasStyleBasedGeneratorArchitecture2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2019},
  month = jun,
  pages = {4396--4405},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00453},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  keywords = {Deep Learning,Image and Video Synthesis,Representation Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\7FINY6Y7\\Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf;C\:\\Users\\hongz\\Zotero\\storage\\F6JGIQE4\\8953766.html}
}
% == BibTeX quality report for style_gan_1_karrasStyleBasedGeneratorArchitecture2019:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("IEEE Xplore")

@inproceedings{style_gan_2_karrasAnalyzingImprovingImage2020,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2020},
  month = jun,
  pages = {8107--8116},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.00813},
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  keywords = {Convolution,Generators,Image resolution,Measurement,Modulation,Standards,Training},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\ZLAMZGGT\\Karras et al. - 2020 - Analyzing and Improving the Image Quality of Style.pdf;C\:\\Users\\hongz\\Zotero\\storage\\RAVGUFBC\\9156570.html}
}
% == BibTeX quality report for style_gan_2_karrasAnalyzingImprovingImage2020:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("IEEE Xplore")

@inproceedings{style_gan_3_nips_karrasAliasFreeGenerativeAdversarial2021,
  title = {Alias-{{Free Generative Adversarial Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2021},
  volume = {34},
  pages = {852--863},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html},
  urldate = {2023-08-23},
  abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\WTZS5VIU\\Karras et al. - 2021 - Alias-Free Generative Adversarial Networks.pdf}
}
% == BibTeX quality report for style_gan_3_nips_karrasAliasFreeGenerativeAdversarial2021:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Neural Information Processing Systems")

@inproceedings{unet_ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} \textendash{} {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\SB3B7489\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}
% == BibTeX quality report for unet_ronnebergerUNetConvolutionalNetworks2015:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Springer Link")

@misc{vae_kingmaAutoEncodingVariationalBayes2013b,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2023-09-05},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\YGHNE8A9\\Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\hongz\\Zotero\\storage\\APKH7FXS\\1312.html}
}
% == BibTeX quality report for vae_kingmaAutoEncodingVariationalBayes2013b:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Version number ("1")

@misc{vgg_simonyanVeryDeepConvolutional2014b,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  month = sep,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/1409.1556v6},
  urldate = {2023-09-06},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\H5DGAYWN\\Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}
% == BibTeX quality report for vgg_simonyanVeryDeepConvolutional2014b:
% ? Possibly abbreviated journal title arXiv.org
% ? Title looks like it was stored in title-case in Zotero

@misc{vqvae_arxiv_oordNeuralDiscreteRepresentation2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2018},
  month = may,
  number = {arXiv:1711.00937},
  eprint = {1711.00937},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.00937},
  url = {http://arxiv.org/abs/1711.00937},
  urldate = {2023-08-19},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\94RCTJFM\\Oord et al. - 2018 - Neural Discrete Representation Learning.pdf;C\:\\Users\\hongz\\Zotero\\storage\\K9CMREBN\\1711.html}
}
% == BibTeX quality report for vqvae_arxiv_oordNeuralDiscreteRepresentation2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Version number ("2")

@inproceedings{vqvae_nips_vandenoordNeuralDiscreteRepresentation2017,
  title = {Neural {{Discrete Representation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{van den Oord}, Aaron and Vinyals, Oriol and {kavukcuoglu}, koray},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html},
  urldate = {2023-08-20},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ``posterior collapse'' -\textemdash{} where the latents are ignored when they are paired with a powerful autoregressive decoder -\textemdash{} typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\BM2IMS7Z\\van den Oord et al. - 2017 - Neural Discrete Representation Learning.pdf}
}
% == BibTeX quality report for vqvae_nips_vandenoordNeuralDiscreteRepresentation2017:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Neural Information Processing Systems")

@inproceedings{vqvae2_nips_razaviGeneratingDiverseHighFidelity2019,
  title = {Generating {{Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Razavi, Ali and {van den Oord}, Aaron and Vinyals, Oriol},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html},
  urldate = {2023-08-20},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before.  We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE  requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of  VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\L4V6CLQU\\Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf}
}
% == BibTeX quality report for vqvae2_nips_razaviGeneratingDiverseHighFidelity2019:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Neural Information Processing Systems")

@inproceedings{wasserstein_gan_icml_arjovskyWassersteinGenerativeAdversarial2017,
  title = {Wasserstein {{Generative Adversarial Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  month = jul,
  pages = {214--223},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
  urldate = {2023-08-23},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
  langid = {english},
  file = {C\:\\Users\\hongz\\Zotero\\storage\\5Z8RIBVW\\Arjovsky et al. - 2017 - Wasserstein Generative Adversarial Networks.pdf;C\:\\Users\\hongz\\Zotero\\storage\\YVILCSEL\\Arjovsky et al. - 2017 - Wasserstein Generative Adversarial Networks.pdf}
}
% == BibTeX quality report for wasserstein_gan_icml_arjovskyWassersteinGenerativeAdversarial2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("International Conference on Machine Learning")
% ? unused Library catalog ("proceedings.mlr.press")
